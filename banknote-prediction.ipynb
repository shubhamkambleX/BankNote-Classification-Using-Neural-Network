{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## importing Libraries","metadata":{"execution":{"iopub.status.busy":"2021-06-03T08:31:36.12977Z","iopub.execute_input":"2021-06-03T08:31:36.130154Z","iopub.status.idle":"2021-06-03T08:31:36.134008Z","shell.execute_reply.started":"2021-06-03T08:31:36.130119Z","shell.execute_reply":"2021-06-03T08:31:36.133125Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-04T05:03:16.542165Z","iopub.execute_input":"2021-06-04T05:03:16.542551Z","iopub.status.idle":"2021-06-04T05:03:16.54965Z","shell.execute_reply.started":"2021-06-04T05:03:16.542519Z","shell.execute_reply":"2021-06-04T05:03:16.548651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## loadind the Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/banknote-authenticationcsv/BankNote_Authentication.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:16.597319Z","iopub.execute_input":"2021-06-04T05:03:16.598573Z","iopub.status.idle":"2021-06-04T05:03:16.631507Z","shell.execute_reply.started":"2021-06-04T05:03:16.598477Z","shell.execute_reply":"2021-06-04T05:03:16.630282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:16.647738Z","iopub.execute_input":"2021-06-04T05:03:16.648676Z","iopub.status.idle":"2021-06-04T05:03:16.660476Z","shell.execute_reply.started":"2021-06-04T05:03:16.648596Z","shell.execute_reply":"2021-06-04T05:03:16.658736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking For Data is Imbalanced Or Not","metadata":{}},{"cell_type":"code","source":"df['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:16.6749Z","iopub.execute_input":"2021-06-04T05:03:16.675458Z","iopub.status.idle":"2021-06-04T05:03:16.687405Z","shell.execute_reply.started":"2021-06-04T05:03:16.675406Z","shell.execute_reply":"2021-06-04T05:03:16.686206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Statistical Information","metadata":{}},{"cell_type":"markdown","source":"**Running the dataset first loads the data before and then prints summary statistics for each variable.\nThe values vary with different means and standard deviations, perhaps some normalization or standardization would be required prior to modeling.**","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:16.702868Z","iopub.execute_input":"2021-06-04T05:03:16.703717Z","iopub.status.idle":"2021-06-04T05:03:16.760095Z","shell.execute_reply.started":"2021-06-04T05:03:16.703665Z","shell.execute_reply":"2021-06-04T05:03:16.75808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Looking Datatypes of the columns","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:16.763115Z","iopub.execute_input":"2021-06-04T05:03:16.764186Z","iopub.status.idle":"2021-06-04T05:03:16.801325Z","shell.execute_reply.started":"2021-06-04T05:03:16.764118Z","shell.execute_reply":"2021-06-04T05:03:16.797738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Looking For Null Values","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:16.804814Z","iopub.execute_input":"2021-06-04T05:03:16.805482Z","iopub.status.idle":"2021-06-04T05:03:16.822728Z","shell.execute_reply.started":"2021-06-04T05:03:16.805433Z","shell.execute_reply":"2021-06-04T05:03:16.821835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratoratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"#### As I can see that while pairplot shows all the histogram in which the first two variables have a Gaussian-like distribution and the next two input variables may have a skewed Gaussian distribution or an exponential distribution. \n#### That have some benefit in using a power transform on each variable in order to make the probability distribution less skewed which will likely improve model performance","metadata":{}},{"cell_type":"code","source":"sns.pairplot(data=df)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:16.82416Z","iopub.execute_input":"2021-06-04T05:03:16.824408Z","iopub.status.idle":"2021-06-04T05:03:20.813381Z","shell.execute_reply.started":"2021-06-04T05:03:16.824384Z","shell.execute_reply":"2021-06-04T05:03:20.812351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df['variance'])","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:20.81556Z","iopub.execute_input":"2021-06-04T05:03:20.815934Z","iopub.status.idle":"2021-06-04T05:03:20.992616Z","shell.execute_reply.started":"2021-06-04T05:03:20.815895Z","shell.execute_reply":"2021-06-04T05:03:20.991669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df['skewness'])","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:20.995506Z","iopub.execute_input":"2021-06-04T05:03:20.99576Z","iopub.status.idle":"2021-06-04T05:03:21.165265Z","shell.execute_reply.started":"2021-06-04T05:03:20.995735Z","shell.execute_reply":"2021-06-04T05:03:21.164432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df['curtosis'])","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:21.166929Z","iopub.execute_input":"2021-06-04T05:03:21.167337Z","iopub.status.idle":"2021-06-04T05:03:21.356705Z","shell.execute_reply.started":"2021-06-04T05:03:21.167295Z","shell.execute_reply":"2021-06-04T05:03:21.353923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df['entropy'])","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:21.358885Z","iopub.execute_input":"2021-06-04T05:03:21.359332Z","iopub.status.idle":"2021-06-04T05:03:21.540125Z","shell.execute_reply.started":"2021-06-04T05:03:21.359289Z","shell.execute_reply":"2021-06-04T05:03:21.539155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Looking for Outliers","metadata":{}},{"cell_type":"code","source":"sns.boxplot(y=df['variance'])","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:21.542071Z","iopub.execute_input":"2021-06-04T05:03:21.542353Z","iopub.status.idle":"2021-06-04T05:03:21.641815Z","shell.execute_reply.started":"2021-06-04T05:03:21.542326Z","shell.execute_reply":"2021-06-04T05:03:21.641142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(y=df['skewness'])","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:21.642974Z","iopub.execute_input":"2021-06-04T05:03:21.643206Z","iopub.status.idle":"2021-06-04T05:03:21.740532Z","shell.execute_reply.started":"2021-06-04T05:03:21.643181Z","shell.execute_reply":"2021-06-04T05:03:21.739547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(y=df['curtosis'])","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:21.74181Z","iopub.execute_input":"2021-06-04T05:03:21.742194Z","iopub.status.idle":"2021-06-04T05:03:21.837217Z","shell.execute_reply.started":"2021-06-04T05:03:21.742153Z","shell.execute_reply":"2021-06-04T05:03:21.836219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(y=df['entropy'])","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:21.838664Z","iopub.execute_input":"2021-06-04T05:03:21.839051Z","iopub.status.idle":"2021-06-04T05:03:21.934473Z","shell.execute_reply.started":"2021-06-04T05:03:21.83901Z","shell.execute_reply":"2021-06-04T05:03:21.93358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the Data into X,y","metadata":{}},{"cell_type":"code","source":"X = df.drop('class',axis=1)\ny = df['class']","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:21.937334Z","iopub.execute_input":"2021-06-04T05:03:21.937591Z","iopub.status.idle":"2021-06-04T05:03:21.943012Z","shell.execute_reply.started":"2021-06-04T05:03:21.937566Z","shell.execute_reply":"2021-06-04T05:03:21.941946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape[1]","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:21.94447Z","iopub.execute_input":"2021-06-04T05:03:21.944848Z","iopub.status.idle":"2021-06-04T05:03:21.963013Z","shell.execute_reply.started":"2021-06-04T05:03:21.944804Z","shell.execute_reply":"2021-06-04T05:03:21.962057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:21.964451Z","iopub.execute_input":"2021-06-04T05:03:21.964845Z","iopub.status.idle":"2021-06-04T05:03:21.974441Z","shell.execute_reply.started":"2021-06-04T05:03:21.964802Z","shell.execute_reply":"2021-06-04T05:03:21.97359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = pd.DataFrame(scaler.fit_transform(X),columns=X.columns)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:21.975822Z","iopub.execute_input":"2021-06-04T05:03:21.976204Z","iopub.status.idle":"2021-06-04T05:03:21.990503Z","shell.execute_reply.started":"2021-06-04T05:03:21.976163Z","shell.execute_reply":"2021-06-04T05:03:21.989684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the data into train ,test split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:21.993455Z","iopub.execute_input":"2021-06-04T05:03:21.993728Z","iopub.status.idle":"2021-06-04T05:03:22.001493Z","shell.execute_reply.started":"2021-06-04T05:03:21.993702Z","shell.execute_reply":"2021-06-04T05:03:22.000447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:22.002692Z","iopub.execute_input":"2021-06-04T05:03:22.002937Z","iopub.status.idle":"2021-06-04T05:03:22.016027Z","shell.execute_reply.started":"2021-06-04T05:03:22.002913Z","shell.execute_reply":"2021-06-04T05:03:22.015398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Train Using Neural Network","metadata":{}},{"cell_type":"markdown","source":"### Neural Network Learning Dynamics\n### I have used a Multilayer Perceptron (MLP) model for the dataset using TensorFlow.\n\n### Given that the dataset is small, a small batch size is probably a good idea, e.g. 16 or 32 rows. Using the Adam version of stochastic gradient descent is a good idea when getting started as it will automatically adapt the learning rate and works well on most datasets.\n\n### Before evaluate models in earnest, it is a good idea to review the learning dynamics and tune the model architecture and learning configuration until we have stable learning dynamics, then look at getting the most out of the model.\n\n### We can do this by using a simple train/test split of the data and review plots of the learning curves. This will help us see if we are over-learning or under-learning; then we can adapt the configuration accordingly.\n\n### First, we must ensure all input variables are floating-point values and encode the target label as integer values 0 and 1.","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:16:45.775418Z","iopub.execute_input":"2021-06-04T05:16:45.775853Z","iopub.status.idle":"2021-06-04T05:16:45.780347Z","shell.execute_reply.started":"2021-06-04T05:16:45.775817Z","shell.execute_reply":"2021-06-04T05:16:45.779292Z"}}},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:22.016792Z","iopub.execute_input":"2021-06-04T05:03:22.01702Z","iopub.status.idle":"2021-06-04T05:03:22.026757Z","shell.execute_reply.started":"2021-06-04T05:03:22.016997Z","shell.execute_reply":"2021-06-04T05:03:22.025857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Used a minimal MLP model. In this case,  used one hidden layer with 10 nodes and one output layer (chosen arbitrarily) and also the ReLU activation function in the hidden layer and the “he_normal” weight initialization, as together, they are a good practice.\n\n#### The output of the model is a sigmoid activation for binary classification and we will minimize binary cross-entropy loss","metadata":{}},{"cell_type":"code","source":"n_features = X.shape[1]\nmodel = tf.keras.Sequential([\ntf.keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)),\ntf.keras.layers.Dense(1, activation='sigmoid')\n])","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:22.028057Z","iopub.execute_input":"2021-06-04T05:03:22.028416Z","iopub.status.idle":"2021-06-04T05:03:22.057675Z","shell.execute_reply.started":"2021-06-04T05:03:22.028375Z","shell.execute_reply":"2021-06-04T05:03:22.05663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:22.060846Z","iopub.execute_input":"2021-06-04T05:03:22.061268Z","iopub.status.idle":"2021-06-04T05:03:22.067164Z","shell.execute_reply.started":"2021-06-04T05:03:22.061224Z","shell.execute_reply":"2021-06-04T05:03:22.066022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"####  fit the model for 50 training epochs (chosen arbitrarily) with a batch size of 32 because it is a small dataset.\n\n#### As fitting the model on raw data, which might be a good idea, but it is an important starting point.","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='binary_crossentropy')","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:22.068911Z","iopub.execute_input":"2021-06-04T05:03:22.069309Z","iopub.status.idle":"2021-06-04T05:03:22.090874Z","shell.execute_reply.started":"2021-06-04T05:03:22.069269Z","shell.execute_reply":"2021-06-04T05:03:22.089905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1, validation_data=(X_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:22.092432Z","iopub.execute_input":"2021-06-04T05:03:22.092901Z","iopub.status.idle":"2021-06-04T05:03:26.958855Z","shell.execute_reply.started":"2021-06-04T05:03:22.092858Z","shell.execute_reply":"2021-06-04T05:03:26.957749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### At the end of training, evaluated the model’s performance on the test dataset and report performance as the classification accuracy.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n# predict test set\nyhat = model.predict_classes(X_test)\n# evaluate predictions\nscore = accuracy_score(y_test, yhat)\nprint('Accuracy: %.3f' % score)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:26.960528Z","iopub.execute_input":"2021-06-04T05:03:26.960846Z","iopub.status.idle":"2021-06-04T05:03:27.068313Z","shell.execute_reply.started":"2021-06-04T05:03:26.960814Z","shell.execute_reply":"2021-06-04T05:03:27.067402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finally, plotted learning curves of the cross-entropy loss on the train and test sets during training.","metadata":{}},{"cell_type":"code","source":"# plot learning curves\nplt.title('Learning Curves')\nplt.xlabel('Epoch')\nplt.ylabel('Cross Entropy')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='val')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:27.069646Z","iopub.execute_input":"2021-06-04T05:03:27.070036Z","iopub.status.idle":"2021-06-04T05:03:27.228019Z","shell.execute_reply.started":"2021-06-04T05:03:27.069986Z","shell.execute_reply":"2021-06-04T05:03:27.22625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(confusion_matrix(y_test,yhat))","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:27.232829Z","iopub.execute_input":"2021-06-04T05:03:27.233256Z","iopub.status.idle":"2021-06-04T05:03:27.2413Z","shell.execute_reply.started":"2021-06-04T05:03:27.233223Z","shell.execute_reply":"2021-06-04T05:03:27.239013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,yhat))","metadata":{"execution":{"iopub.status.busy":"2021-06-04T05:03:27.24282Z","iopub.execute_input":"2021-06-04T05:03:27.24338Z","iopub.status.idle":"2021-06-04T05:03:27.26466Z","shell.execute_reply.started":"2021-06-04T05:03:27.243335Z","shell.execute_reply":"2021-06-04T05:03:27.263108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}